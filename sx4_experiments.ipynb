{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_rounding(x):\n",
    "    # takes the integer part of the number\n",
    "    x_int = x.astype(np.int32)\n",
    "    # takes the fractional part\n",
    "    x_frac = np.abs(x - x_int)\n",
    "\n",
    "    # generate a random number\n",
    "    rng = np.random.random(x_int.shape)\n",
    "\n",
    "    # if the frac is grater... for positive cases\n",
    "    rounded_pos = np.where(x_frac > rng, x_int + 1, x_int)\n",
    "\n",
    "    # if the grac is greate... for negative cases\n",
    "    rounded_neg = np.where(x_frac > rng, x_int - 1, x_int)\n",
    "\n",
    "    # select the rounded according to the signal\n",
    "    rounded = np.where(x < 0, rounded_neg, rounded_pos)\n",
    "    \n",
    "    return rounded\n",
    "\n",
    "x = np.random.normal(0, 5, (5)) \n",
    "print(\"float \", x)\n",
    "rx = 0\n",
    "it = 100\n",
    "for _ in range(it):\n",
    "    rx += stochastic_rounding(x)\n",
    "\n",
    "rx = rx.astype(np.float32) / it\n",
    "print(\"stchr0\", stochastic_rounding(x))\n",
    "print(\"stchr1\", rx)\n",
    "print(\"round \", np.around(x).astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quantize(x, round_stoch = True):\n",
    "    \"\"\" exponentiation and quantization function \"\"\"\n",
    "\n",
    "    # just to avoid numerical problems\n",
    "    eps = 1e-8\n",
    "\n",
    "    # extract the signal\n",
    "    s = np.sign(x)\n",
    "\n",
    "    # takes the abs\n",
    "    abs_x = np.abs(x)\n",
    "\n",
    "    cliped_abs_x = np.where(abs_x < eps, eps, abs_x) # clip the min value of abs. (this is just for avoid numercal problems)\n",
    "    cliped_abs_x = np.where(cliped_abs_x > 1, 1, cliped_abs_x) # clip the max value of DN \n",
    "\n",
    "    # gets the exponent with base 2\n",
    "    exp = np.log2(cliped_abs_x)\n",
    "\n",
    "    # round to nearest and cast to int (use stochastic rounding)\n",
    "    if round_stoch:\n",
    "        round_exp = stochastic_rounding(exp)\n",
    "    else:\n",
    "        round_exp = (np.round(exp)).astype(np.int32)\n",
    "\n",
    "\n",
    "    # stochastic zero\n",
    "    \n",
    "    # detect underflow\n",
    "    underflow = np.where(round_exp < -7, 1, 0)\n",
    "\n",
    "    # clip expoent in -7\n",
    "    clip_exp = np.where(underflow, -7, round_exp)\n",
    "    \n",
    "    # randomize the signal\n",
    "    s = np.where(np.logical_and(np.random.random(round_exp.shape) < 0.5, underflow), -s, s) \n",
    "    \n",
    "    # convert to float32 again\n",
    "    qx = s * np.power(2., clip_exp)\n",
    "    return qx\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    x = np.random.normal(0, 0.2, (5)) \n",
    "    print(\"float \", x)\n",
    "    print(\"quant0\", quantize(x))\n",
    "    print(\"quant1\", quantize(x))\n",
    "    print(\"quant2\", quantize(x))    \n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([0,1,0,1])\n",
    "b = np.array([1,1,0,0])\n",
    "\n",
    "print(np.logical_and(a, b))\n",
    "\n",
    "print(np.sign(-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# range of values from -2,2 \n",
    "float_values = np.arange(-1, 1, 0.001)\n",
    "\n",
    "\n",
    "sx4_values = quantize(float_values, True)\n",
    "sx4_round_values = quantize(float_values, False)\n",
    "\n",
    "\n",
    "sx4_over_time = quantize(float_values, True)\n",
    "for _ in range(100):\n",
    "    sx4_over_time += quantize(float_values, True)\n",
    "sx4_over_time /= 100\n",
    "\n",
    "\n",
    "print(float_values.shape)\n",
    "print(sx4_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20), dpi=300)\n",
    "plt.plot(sx4_values)\n",
    "plt.plot(sx4_over_time)\n",
    "plt.plot(sx4_round_values)\n",
    "plt.plot(float_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "a = np.random.randn(784, 256) * np.sqrt(2/784)\n",
    "\n",
    "plt.hist(np.ravel(a), bins=64)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantizer\n",
    "import cupy as cp\n",
    "\n",
    "a = cp.array(2.5)\n",
    "b = cp.array(4.3)\n",
    "c = cp.array(a * b)\n",
    "d = cp.array(quantizer.quantize_po2(a) * quantizer.quantize_po2(b))\n",
    "e = cp.array(quantizer.quantize_po2(c))\n",
    "f = cp.array(quantizer.quantize_po2(d))\n",
    "h = quantizer.quantize_po2(a)\n",
    "i = quantizer.quantize_po2(b)\n",
    "\n",
    "print(\"a=%f, b%f, c%f, d%f, e%f, f%f, h%f, i%f\"%(a, b, c, d, e, f, h, i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tests with tensorflow convolution foward and backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 416, 416, 16)\n",
      "(32, 416, 416, 64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomConvLayer():\n",
    "    def __init__(self, filters, kernel_size, input_channels, strides=[1,1,1,1], padding='SAME'):        \n",
    "        \n",
    "        self.filters = tf.constant(np.random.randn(kernel_size, kernel_size, input_channels, filters), dtype=tf.float32)\n",
    "        self.bias = tf.constant(np.random.randn(filters), dtype=tf.float32)\n",
    "\n",
    "        # other atributes        \n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        z = tf.nn.bias_add(tf.nn.conv2d(input=inputs, filters=self.filters, strides=self.strides, padding=self.padding), self.bias)\n",
    "        return z\n",
    "\n",
    "\n",
    "    def backward(self, dz, learning_rate):\n",
    "        dx = tf.compat.v1.nn.conv2d_backprop_input(tf.shape(self.inputs), self.filters, dz, strides=self.strides, padding=self.padding)\n",
    "        dw = tf.compat.v1.nn.conv2d_backprop_filter(self.inputs, tf.shape(self.filters), dz, strides=self.strides, padding=self.padding)\n",
    "        db = tf.reduce_sum(dz, (0,1,2), True)\n",
    "\n",
    "        # update weights\n",
    "        self.filters = self.filters - learning_rate * dw\n",
    "        self.bias = self.bias - learning_rate * db\n",
    "\n",
    "        return dx\n",
    "    \n",
    "x = tf.random.normal((32, 416, 416, 64), dtype=tf.float32)\n",
    "conv2D = CustomConvLayer(16, 3, 64)\n",
    "z = conv2D.forward(x)\n",
    "print(z.shape)\n",
    "\n",
    "dx = conv2D.backward(z, 0.0001)\n",
    "print(dx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from dataset import load_mnist\n",
    "\n",
    "# Exemplo de uso da camada personalizada\n",
    "input_shape = (28, 28, 3)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='SAME'),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='SAME'),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),    \n",
    "    tf.keras.layers.Dense(256),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Dense(256),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Dense(10),        \n",
    "    tf.keras.layers.Softmax()    \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = tf.cast(tf.expand_dims(x_train, -1), tf.float32)\n",
    "x_test = tf.cast(tf.expand_dims(x_test, -1), tf.float32)\n",
    "y_train = tf.one_hot(y_train, 10, 1., 0.)\n",
    "y_test = tf.one_hot(y_test, 10, 1., 0.)\n",
    "\n",
    "# Compilação e treinamento do modelo\n",
    "optimizer = tf.keras.optimizers.SGD(0.001)\n",
    "batch_size = 256\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'], run_eagerly=True)\n",
    "model.build([batch_size] + x_train.shape[1:])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
